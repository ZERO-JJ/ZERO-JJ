from keras.layers import Dense, Dropout
from keras.models import Sequential
import pandas as pd
import numpy as np
from keras.layers import Flatten
import matplotlib.pyplot as plt
from sklearn.preprocessing import scale
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import StratifiedKFold
from keras.models import Model
from keras.models import Sequential
from keras.layers import Dense,Input,Dropout
from keras.layers import Conv1D, MaxPooling1D, MaxPooling2D,Conv2D
import utils
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import accuracy_score, precision_score, recall_score,  matthews_corrcoef, f1_score  
from sklearn.metrics import confusion_matrix 
    
def calculate_performance(y_true, y_pred):
    # Calculate evaluation metrics from confusion matrix
    CC=confusion_matrix(y_true, y_pred)
    acc = accuracy_score(y_true, y_pred)   
    sensitivity=CC[0,0]/(CC[0,0]+CC[0,1])  # Sensitivity/Recall
    mcc = matthews_corrcoef(y_true, y_pred)
    Sp=CC[1,1]/(CC[1,0]+CC[1,1])           # Specificity
    return acc,sensitivity,mcc,Sp

def categorical_probas_to_classes(p):
    # Convert probability vectors to class labels
    return np.argmax(p, axis=1)

def to_categorical(y, num_classes=None, dtype='float32'):
    # Modified keras.to_categorical for compatibility
    y = np.array(y, dtype='int')
    input_shape = y.shape
    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:
        input_shape = tuple(input_shape[:-1])
    y = y.ravel()
    if not num_classes:
        num_classes = np.max(y) + 1
    n = y.shape[0]
    categorical = np.zeros((n, num_classes), dtype=dtype)
    categorical[np.arange(n), y] = 1
    output_shape = input_shape + (num_classes,)
    categorical = np.reshape(categorical, output_shape)
    return categorical

from sklearn import datasets
from keras.layers import *
import keras.backend as K
from keras import Model
from keras.utils import plot_model,to_categorical
from sklearn.utils import shuffle

# Load and preprocess dataset
data_=pd.read_csv(r"C:\Users\魏\Desktop\桌面\Human1.csv")
data=np.array(data_)
data=data[:,1:]  # Remove index column
[m1,n1]=np.shape(data)
label1=np.ones((int(m1/2),1))  # Positive class labels
label2=np.zeros((int(m1/2),1)) # Negative class labels
label=np.append(label1,label2)
shu=scale(data)  # Standardize features
X1=shu
y=label
X=np.reshape(X1,(m1,n1))  # Reshape for model input
sepscores = []

# Initialize placeholder arrays
ytest=np.ones((1,2))*0.5
yscore=np.ones((1,2))*0.5

from sklearn.model_selection import ParameterSampler
import random

# Define hyperparameter search space
param_dist = {
    'reshape_dim': [6,7,8],          # Feature matrix reshape dimensions
    'conv_kernel': [3,4,5],         # Convolution kernel size
    'conv_stride': [1,2,3],         # Convolution stride length
    'activation': ['relu', 'selu'], # Activation function type
    'dense_layers': [1, 2, 3],      # Number of dense layers
    'dense_units': [4,8,16,32]      # Units per dense layer
}

def build_model(fea_cnt, numb, params):
    # Neural network architecture builder
    inputs = Input(shape=(fea_cnt,))
    
    try:
        # First dense layer for feature transformation
        dense_units = params['reshape_dim']**2
        embds = Dense(dense_units, activation=params['activation'])(inputs)
        
        # Reshape to 2D feature matrix
        embds = Reshape((params['reshape_dim'], params['reshape_dim'], 1))(embds)
        
        # Convolutional layer
        embds = Conv2D(32, (params['conv_kernel'], params['conv_kernel']), 
                      strides=params['conv_stride'], 
                      padding='same', 
                      activation=params['activation'])(embds)
        
        # Pooling layer
        embds = MaxPooling2D(pool_size=(2,2))(embds)
        embds = Flatten()(embds)
        
        # Stack dense layers
        for _ in range(params['dense_layers']):
            embds = Dense(params['dense_units'], activation=params['activation'])(embds)
            embds = Dropout(0.3)(embds)
            
        # Output layer
        outputs = Dense(numb, activation='sigmoid')(embds)
        
        model = Model(inputs=inputs, outputs=outputs)
        model.compile(loss='categorical_crossentropy', 
                     optimizer='adam', 
                     metrics=['accuracy'])
        return model
    except Exception as e:
        print(f"Invalid parameters {params}: {str(e)}")
        return None

def random_search(X, y, n_iter=20):
    # Hyperparameter optimization using random search
    best_params = None
    best_score = 0
    param_list = list(ParameterSampler(param_dist, n_iter=n_iter))
    
    for params in param_list:
        try:
            # Parameter feasibility check
            input_size = params['reshape_dim']
            conv_output = (input_size + params['conv_stride'] - 1) // params['conv_stride']
            pool_output = (conv_output + 1) // 2
            if pool_output < 2:
                raise ValueError("Invalid feature dimensions after pooling")
                
            fold_scores = []
            skf = StratifiedKFold(n_splits=5)  # 5-fold cross-validation
            
            for train, test in skf.split(X, y):
                model = build_model(X.shape[1], 2, params)
                if model is None:
                    continue
                
                # Train with early stopping
                history = model.fit(
                    X[train], to_categorical(y[train]),
                    epochs=15,  
                    batch_size=32,
                    verbose=0,
                    validation_split=0.1
                )
                
                # Evaluate model
                y_pred = model.predict(X[test])
                y_class = np.argmax(y_pred, axis=1)
                acc, _, mcc, Sp = calculate_performance(y[test], y_class)
                fold_scores.append(acc + mcc)  # Combined evaluation metric
                
            mean_score = np.mean(fold_scores)
            if mean_score > best_score:
                best_score = mean_score
                best_params = params
                print(f"New best: {best_score:.4f} with {params}")
                
        except Exception as e:
            print(f"Skipping {params} due to {str(e)}")
            continue
            
    return best_params, best_score

# Execute hyperparameter search
best_params, best_score = random_search(X, y, n_iter=30)
print(f"\nOptimal Parameters: {best_params}")
print(f"Best Composite Score: {best_score:.4f}")

# Build final model with optimal parameters
final_model = build_model(X.shape[1], 2, best_params)
final_model.summary()

# Cross-validation with optimal parameters
skf = StratifiedKFold(n_splits=10)
sepscores = []

for train, test in skf.split(X,y):
    y_train = to_categorical(y[train])
    final_model.fit(X[train], y_train, epochs=30, verbose=0)
    
    y_score = final_model.predict(X[test])
    y_class = np.argmax(y_score, axis=1)
    
    # Calculate performance metrics
    acc, sensitivity, mcc, Sp = calculate_performance(y[test], y_class)
    fpr, tpr, _ = roc_curve(y[test], y_score[:,1])
    roc_auc = auc(fpr, tpr)
    sepscores.append([acc, sensitivity, mcc, Sp, roc_auc])

# Output final performance metrics
scores = np.array(sepscores)
print("\nFinal Performance with Optimal Parameters:")
print("acc=%.2f%% (+/- %.2f%%)" % (np.mean(scores, axis=0)[0]*100, np.std(scores, axis=0)[0]*100))
print("sensitivity=%.2f%% (+/- %.2f%%)" % (np.mean(scores, axis=0)[1]*100, np.std(scores, axis=0)[1]*100))
print("mcc=%.2f%% (+/- %.2f%%)" % (np.mean(scores, axis=0)[2]*100, np.std(scores, axis=0)[2]*100))
print("Sp=%.2f%% (+/- %.2f%%)" % (np.mean(scores, axis=0)[3]*100, np.std(scores, axis=0)[3]*100))
print("roc_auc=%.2f%% (+/- %.2f%%)" % (np.mean(scores, axis=0)[4]*100, np.std(scores, axis=0)[4]*100))

